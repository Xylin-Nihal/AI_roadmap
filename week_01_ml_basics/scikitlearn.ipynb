{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6988af33",
   "metadata": {},
   "source": [
    "# Scikit-learn Course: Learn Machine Learning from Scratch\n",
    "Welcome to the scikit-learn course! This notebook will guide you through the basics and advanced features of the scikit-learn library, a powerful Python framework for machine learning. Each section includes code snippets and explanations to help you understand and apply machine learning concepts using scikit-learn.\n",
    "\n",
    "## What is scikit-learn?\n",
    "Scikit-learn is an open-source Python library that provides simple and efficient tools for data mining and data analysis. It is built on top of NumPy, SciPy, and matplotlib, and is widely used for implementing machine learning algorithms.\n",
    "\n",
    "## What will you learn?\n",
    "- How to install and import scikit-learn\n",
    "- Loading datasets\n",
    "- Data preprocessing\n",
    "- Building and training models\n",
    "- Evaluating models\n",
    "- Hyperparameter tuning\n",
    "- Pipelines and advanced topics\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4269b8",
   "metadata": {},
   "source": [
    "## 1. Installing and Importing scikit-learn\n",
    "To use scikit-learn, you first need to install it. You can install it using pip. After installation, you can import it in your Python code.\n",
    "\n",
    "- `pip install scikit-learn` installs the library.\n",
    "- `import sklearn` imports the main package. Usually, you import specific modules like `from sklearn import datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac86c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Install scikit-learn (uncomment the line below if not already installed)\n",
    "# !pip install scikit-learn numpy pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('scikit-learn version:', sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7932f09",
   "metadata": {},
   "source": [
    "## 2. Loading Datasets\n",
    "Scikit-learn comes with several built-in datasets for practice, such as the iris and digits datasets. You can also load your own data from CSV or Excel files.\n",
    "\n",
    "- `sklearn.datasets.load_iris()` loads the famous iris flower dataset.\n",
    "- `sklearn.datasets.load_digits()` loads handwritten digits data.\n",
    "- For your own data, use pandas: `pd.read_csv('yourfile.csv')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a08fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys of iris dataset: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n",
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "First 5 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print('Keys of iris dataset:', iris.keys())\n",
    "print('Feature names:', iris.feature_names)\n",
    "print('Target names:', iris.target_names)\n",
    "print('First 5 rows of data:')\n",
    "print(iris.data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a77a3",
   "metadata": {},
   "source": [
    "## 3. Splitting Data into Training and Test Sets\n",
    "To evaluate a machine learning model, you need to split your data into a training set (to train the model) and a test set (to evaluate it).\n",
    "Scikit-learn provides `train_test_split` for this purpose.\n",
    "\n",
    "- `train_test_split` randomly splits data into training and test sets.\n",
    "- Typical split: 70-80% for training, 20-30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa32720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (120, 4)\n",
      "Test set size: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('Training set size:', X_train.shape)\n",
    "print('Test set size:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3606a",
   "metadata": {},
   "source": [
    "## 4. Training a Simple Model: k-Nearest Neighbors (KNN)\n",
    "Let's train a simple classifier using the k-Nearest Neighbors algorithm. KNN is a basic classification algorithm that predicts the class of a data point based on the classes of its nearest neighbors.\n",
    "\n",
    "- `KNeighborsClassifier` is used for classification tasks.\n",
    "- `fit()` trains the model on the training data.\n",
    "- `predict()` makes predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bbafe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print('Predicted labels:', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e207ed0",
   "metadata": {},
   "source": [
    "## 5. Evaluating the Model\n",
    "After training a model, you need to evaluate its performance. The most common metric for classification is accuracy, which measures the proportion of correct predictions.\n",
    "\n",
    "- `accuracy_score` computes the accuracy of the model.\n",
    "- You can also use a confusion matrix to see details of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa36514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658804e0",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling and Preprocessing\n",
    "Many machine learning algorithms perform better when features are on a similar scale. Scikit-learn provides tools for scaling and preprocessing data.\n",
    "\n",
    "- `StandardScaler` standardizes features by removing the mean and scaling to unit variance.\n",
    "- Always fit the scaler on the training data and transform both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37c0f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of scaled training data:\n",
      "[[-1.47393679  1.20365799 -1.56253475 -1.31260282]\n",
      " [-0.13307079  2.99237573 -1.27600637 -1.04563275]\n",
      " [ 1.08589829  0.08570939  0.38585821  0.28921757]\n",
      " [-1.23014297  0.75647855 -1.2187007  -1.31260282]\n",
      " [-1.7177306   0.30929911 -1.39061772 -1.31260282]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print('First 5 rows of scaled training data:')\n",
    "print(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c3fa3",
   "metadata": {},
   "source": [
    "## 7. Pipelines: Combining Preprocessing and Modeling\n",
    "Scikit-learn pipelines allow you to chain preprocessing and modeling steps together, making your code cleaner and less error-prone.\n",
    "\n",
    "- `Pipeline` lets you define a sequence of steps (e.g., scaling, then classification).\n",
    "- This ensures that all steps are applied consistently during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1873ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=3))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_pipe = pipeline.predict(X_test)\n",
    "accuracy_pipe = accuracy_score(y_test, y_pred_pipe)\n",
    "print('Pipeline accuracy:', accuracy_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98e15a",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning with GridSearchCV\n",
    "Choosing the best parameters for your model can improve its performance. Scikit-learn provides `GridSearchCV` to search for the best hyperparameters automatically.\n",
    "\n",
    "- `GridSearchCV` tries different combinations of parameters and finds the best one based on cross-validation.\n",
    "- You define a parameter grid and the model to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa9d9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'knn__n_neighbors': 3}\n",
      "Best cross-validation score: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'knn__n_neighbors': [1, 3, 5, 7, 9]}\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Best cross-validation score:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b489bb3",
   "metadata": {},
   "source": [
    "## 9. Trying Different Models: Classification and Regression\n",
    "Scikit-learn supports many algorithms for both classification and regression tasks. You can easily switch between models by changing the estimator in your pipeline.\n",
    "\n",
    "- For classification: `LogisticRegression`, `DecisionTreeClassifier`, `RandomForestClassifier`, etc.\n",
    "- For regression: `LinearRegression`, `DecisionTreeRegressor`, `RandomForestRegressor`, etc.\n",
    "\n",
    "Let's see an example with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f27ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg_pipeline.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "print('Logistic Regression accuracy:', accuracy_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09313203",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation with Cross-Validation\n",
    "Cross-validation is a technique to assess how well your model generalizes to unseen data. It splits the data into multiple parts (folds), trains the model on some folds, and tests it on the remaining fold(s).\n",
    "- `cross_val_score` computes scores for each fold and returns the results.\n",
    "- Commonly used with 5 or 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db11f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96666667 1.         0.93333333 0.9        1.        ]\n",
      "Mean cross-validation score: 0.9600000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(logreg_pipeline, X, y, cv=5)\n",
    "print('Cross-validation scores:', scores)\n",
    "print('Mean cross-validation score:', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0836b90",
   "metadata": {},
   "source": [
    "## 11. Saving and Loading Models\n",
    "After training a model, you may want to save it for later use. Scikit-learn provides utilities to save and load models using the `joblib` library.\n",
    "\n",
    "- `joblib.dump()` saves the model to a file.\n",
    "- `joblib.load()` loads the model from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Save the trained logistic regression pipeline\n",
    "joblib.dump(logreg_pipeline, 'logreg_pipeline.joblib')\n",
    "# Load the model back\n",
    "loaded_model = joblib.load('logreg_pipeline.joblib')\n",
    "print('Loaded model accuracy:', loaded_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be4b5d",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "You have now learned the basics of scikit-learn, including how to load data, preprocess it, train and evaluate models, use pipelines, tune hyperparameters, and save/load models.\n",
    "\n",
    "## Next Steps\n",
    "- Explore more datasets and algorithms in scikit-learn.\n",
    "- Try regression tasks with `LinearRegression` or `RandomForestRegressor`.\n",
    "- Learn about advanced topics like ensemble methods, feature selection, and custom transformers.\n",
    "- Read the [scikit-learn documentation](https://scikit-learn.org/stable/documentation.html) for more details and examples.\n",
    "\n",
    "Happy Learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
